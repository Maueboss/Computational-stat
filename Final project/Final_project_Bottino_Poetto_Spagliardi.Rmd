---
title: "Final_project_Bottino_Poetto_Spagliardi"
author: "Manuel Bottino, Patrick Poetto, Jacopo Spagliardi"
date: "`r Sys.Date()`"

output:
  pdf_document: 
    toc: true
    number_sections: true
    latex_engine: pdflatex
fig_caption: yes
urlcolor: blue
header-includes:
  - \usepackage{amssymb}
editor_options: 
  markdown: 
    wrap: 72
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(magrittr)     #Pipe operator for easier data manipulation.
library(dplyr)        #Data manipulation and transformation.
library(ggplot2)      #Data visualization using the grammar of graphics.
library(gridExtra)    #Arrange multiple plots on a grid.
library(ggpubr)       #Extension of ggplot2 for publication-ready graphics.
library(bookdown)     #Authoring books and documents with R Markdown.
library(plotly)       #Interactive and web-based visualizations.
library(rstanarm)     #Bayesian applied regression modeling.
library(Cairo)        #Graphics device for creating high-quality images.
library(rgl)          #3D visualization using OpenGL.
library(plot3D)       #3D scatterplots and surface plots.
library(effects)      # effects - Effect displays for statistical models.
library(MASS)         #Functions and datasets for applied statistics.
library(car)          #Companion to Applied Regression textbook.
library(faraway)      #Functions and datasets for regression and ANOVA.
library(leaps)        #Feature selection and regression subset selection.
library(ROCR)         #Performance evaluation of binary classifiers.
library(RColorBrewer) #Color palettes for data visualization.
library(ISLR2)        #Data and functions for the book "A.I.T.S.L.W.A.I.R" 
library(e1071)        #Various functions for statistical learning/classification.
library(class)        #Functions for classification.
library(reshape2)     #Reshape and manipulate data.
library(sads)         #Species abundance distributions.
library(knitr)        #Dynamic report generation with R Markdown.
library(latex2exp)    #LaTeX expressions in plots.
library(GGally)       #Extension to ggplot2 for matrix-based plots.
library(combinat)     #Permutations, combinations, and combinatorial datasets.
library(superheat)    #Heatmaps with improved aesthetics and interactivity.
library(stringr)      #String manipulation functions.
library(purrr)        #Functional programming with vectors and lists.
library(tidyr)        #Tidy data principles for data cleaning.
library(lubridate)    #Easy handling of dates and times.
library(ggthemes)     #Additional themes for ggplot2.
library(kableExtra)   #Create complex tables using 'kable' and 'knitr'.
library(covr)         #Test coverage reports for R packages.
library(doParallel)   #Parallel computing using the foreach framework.
library(foreach)      #Looping construct for iterating over elements.
library(caret)        #Classification and regression training package.
library(rpart)        #Recursive partitioning for decision trees.
library(ipred)        #Improved predictive models.
library(rpart.plot)   #Plotting decision trees from rpart objects.
library(tidymodels)   #Tidyverse-style modeling framework.
library(tidyr)        #Tidy data principles for data cleaning.
library(e1071)        #for calculating variable importance
library(caret)        #for general model fitting
library(rpart)        #for fitting decision trees
library(ipred)        #
library(randomForest) #
library(RefManageR)   #
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# References

```{r echo=FALSE, results='asis'}
bibliography <- ReadBib("Final_references.bib")
options <- BibOptions(bib.style = "authoryear", cite.style = "latex")
print(bibliography, .opts = options)
```

\newpage

# Review

## Why it works
First of all let's introduce the general idea of bagging. Given a dataset $L=\{(x_n,y_n),\ n=1, \ldots ,N\}$ we try to improve a predicting procedure in a "naive" way, ideally we would like to have a sequence of datasets $\{L_k\}$ of training sets each containing  N independent observations, then take the average of the sequence $\{\phi(x, L_k)\}$.

\begin{equation}
\phi_A(x,P)=E_L[\phi(x,L)]\label{eq:agg}
\end{equation}.

Because $\phi_A$ depends not only on x but also the underlying probability distribution $P$ from which L is drawn. Since we are missing this last information the only tool we are left with is our dataset L, we instead use $P_L$ the bootstrap approximation of $P$, it can be defined as the probability mass function with equal probability of extracting each element of the dataset $(x_n,y_n)$. Finally we can define the bootstrap aggregate predictor as:

\begin{equation}
\phi_B (x)=\phi_A(x,P_L)\label{eq:boot}
\end{equation}.

In order to understand why bagging works theoretically it can be proved that mean-squared error (MSE) of $\phi_A(x)$ is lower than the mean-squared error averaged over L of $\phi(x,L)$. How much lower the two sides are depends on the inequality: 

\begin{equation}
[E_L\phi(x,L)]^2\leq E_L\phi^2(x,L)\label{eq:ineq}
\end{equation}

This result is true taking advantage of the Jensen’s inequality for the specific case in which $g(X)=X^2$, this function is convex ($g''>0$), thus $E[Z^2] \geq (E[Z])^2$


## Instability
The inequality \@ref(eq:ineq) is a nice starting point to explain what role instability plays. In fact, If $\phi(x, L)$ does not change too much with replicate L the two sides will be nearly equal, and aggregation will not help. The more highly variable the $\phi(x, L)$ are, the more improvement aggregation may produce. Applying this reasoning to our  $\phi_B (x)$, if the procedure is unstable, it can give improvement through aggregation. However, if the procedure is stable, then $\phi_B (x)=\phi_A(x,P_L)$ will not be as accurate for data drawn from $P$ as $\phi_A(X, P) \sim \phi(x,L)$.

Let's see how this concept can be translated when we look at a more specific context, like classification. In this instance the predictor $\phi(\pmb x,L)$ predicts a label $j \in \{1,..., J\}$. We first define $Q(j|x)=P(\phi(x,L)=j)$, over many independent replicates of the learning set $L$, $\phi$ predicts class label $j$ at input $x$ with relative frequency $Q(j|x)$, and let $P(j|x)$ be the probability that input x generates class $j$.
At this point we can set $\phi^*(x)= \arg \max _j P(j|x)$ (the Bayes predictor) which leads to the following expression for $Q (j |x)$:

\[
\begin{cases}
1 \ if \ P(j|x)=max_iP(i|x) \\
0 \ elsewhere
\end{cases}
\]

Now we have all the ingredients to show the maximum classification rate where:
\begin{equation}
r^*=\int \max_j P(j|x) P_X(dx)\label{eq:class_max}
\end{equation}

where $P_X(dx)$ is the probability distribution of X.

If we know focus on the aggregate of $\phi$ and define it following the procedure described above $\phi_A(x)=\arg \max_j Q(j|x)$. We have the maximum attainable correct-classification rate of $\phi(x)$, we are missing the corret-classification for x, which for $\phi_A(x)$ is $\sum_j I(\arg \max_j Q(j|x)=j)P(j|x)$ Where $I$ is the indicator function. 
*Putting all the pieces together* the correct-classification rate for $\phi_A(x)$ is:
\begin{equation}
r_A=\int_{\pmb x \in C} max_j P(j| \pmb x) P_X(d \pmb x)+ \int_{\pmb x \in C’} [\sum_j(I(\phi_A(\pmb x)=j)P(j|\pmb x)] P_X(\pmb x)
\label{eq:class_res}
\end{equation}
Even if $\phi$ is order correct at $x$ its correct classification rate can be far from optimal, but $\phi_A$ is optimal. Only if $\phi$ is order-correct for most input of x (it is good), then aggregation can transform it into a nearly optimal predictor. On the other hand, unlike the numerical prediction situation, poor predictors can be transformed into worse ones if we use bagging.

## Model - Decision Tree-Based Methods
Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model, say a constant, in each one. This is a simple yet powerful procedure since it is able to disentangle a model into simpler and smaller models and describe his features more accurately than global models do basically using binary conditional clustering. The geometric perspective described before can be seen as a tree where data are run and at each node a test is conducted to see what is the path a covariate should follow until reaching a leaf, which represents the final prediction explained by the constant model. 
For example, let's say we have $p$ inputs and a response, for each of $N$ observations: that is, $(x_i,y_i)$ for $i = 1,2,...,N$, with $x_i = (x_{i1},x_{i2},...,x_{ip})$. The algorithm has decide on the splitting variables and split points, as well as what shape the tree
should have. Suppose first that we have a partition into $M$ regions $R_1, R_2, ..., R_M$, and we model the response as a constant $c_m$ in each region: As a criterion for optimal partitional we can minimize the sum of squares $\sum(y_i-f(x_i))^2$. In this way the best $\hat{c}_m$ is just the average of $y_i$ in region $R_m$: $\hat{c}_m=av(y_i | x_i \in R_m)$. Our model will be then: $f(x)=\sum_{m=1}^{M} c_m \cdot I(x \in R_m)$.

Now finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible so we can set up a CART (classification and regression tree) algorithm starting with the data, a splitting variable $j$, a split point $s$, and defining the half planes as: $R_1(j,s) = \{X\mid X_j \leq s\}$ and $R_2(j,s) = \{X\mid X_j > s\}$. Then we seek $j$ and $s$ that solve 

```{=latex}
\begin{center}
$\underset{j,s}{\min}\left[\underset{c_1}{\min}\sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \underset{c_2}{\min} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2\right]$
\end{center}
```

For any choice $j$ and $s$, the inner minimization is solved by: 

```{=latex}
\begin{center}
$\hat{c}_1 = \text{av}(y_i \,|\, x_i \in R_1(j, s)) \quad \text{and} \quad \hat{c}_2 = \text{av}(y_i \,|\, x_i \in R_2(j, s))$
\end{center}
```

For each $j$, the split point $s$ can be found very quickly and hence determination of the best pair (j, s) is feasible by brute force. Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. Then this process is repeated on all of the resulting regions.
The question now become: how large should we grow the three? It is pretty straightforward that too many nodes (splits) may overfit the data while doing vice versa may end up not being able to capture them well, resulting in misprediction. One strategy could be to set a lower threshold for the decrease of the sum of squares and stop the splitting when this is reached. However this strategy is too short sighted since a seemingly worthless split may lead to a very good one below. A more robust strategy may be to do kind of the opposite: grow a very large tree and then use a cost-complexity pruning criterion to collapse one internal node at a time from the full tree until the single node tree so that we find a sequence. It is intuitive that the optimal tree must be somewhere in the sequence. Now, with a cross validation selection method, we can find the actual optimal tree just minimizing the cross validated sum of squares. 

This is how the CART algorithm for growing decision trees basically works. Note that decision trees are divided in classification and regression trees if the response is a factor or a numerical or continous variable. 
Below it is proposed a very basic implementation of a CART algorithm using mean squared error cost complexity criterion. The algorithm is not implemented to prune the tree. 

```{r eval=FALSE, include=FALSE}
#implementazione CART
# Definizione della funzione di costo per la suddivisione
calculate_cost <- function(y) {
  n <- length(y)
  mean_squared_error <- sum((y - mean(y))^2) / n
  return(mean_squared_error)
}

# Definizione della funzione per la ricerca della miglior suddivisione
find_best_split <- function(X, y) {
  best_cost <- Inf
  best_feature <- NULL
  best_threshold <- NULL

  for (i in 1:ncol(X)) {
    feature_values <- unique(X[, i])
    for (value in feature_values) {
      left_indices <- X[, i] <= value
      right_indices <- X[, i] > value

      left_y <- y[left_indices]
      right_y <- y[right_indices]

      cost <- calculate_cost(left_y) + calculate_cost(right_y)

      if (cost < best_cost) {
        best_cost <- cost
        best_feature <- i
        best_threshold <- value
      }
    }
  }

  return(list(feature = best_feature, threshold = best_threshold))
}

# Definizione della classe dell'albero di regressione
class RegressionTree {
  feature <- NULL
  threshold <- NULL
  value <- NULL
  left_child <- NULL
  right_child <- NULL
}

# Definizione della funzione per la costruzione dell'albero di regressione
build_regression_tree <- function(X, y, max_depth) {
  if (length(unique(y)) == 1 || max_depth == 0) {
    leaf <- RegressionTree()
    leaf$value <- mean(y)
    return(leaf)
  }

  split <- find_best_split(X, y)
  feature <- split$feature
  threshold <- split$threshold

  left_indices <- X[, feature] <= threshold
  right_indices <- X[, feature] > threshold

  left_X <- X[left_indices, ]
  left_y <- y[left_indices]

  right_X <- X[right_indices, ]
  right_y <- y[right_indices]

  node <- RegressionTree()
  node$feature <- feature
  node$threshold <- threshold
  node$left_child <- build_regression_tree(left_X, left_y, max_depth - 1)
  node$right_child <- build_regression_tree(right_X, right_y, max_depth - 1)

  return(node)
}

# Utilizzo dell'algoritmo CART per costruire un albero di regressione
data <- data.frame(
  age = c(25, 30, 35, 40),
  income = c(50000, 60000, 70000, 80000),
  outcome = c(100, 150, 200, 250)
)

X <- data[, c("age", "income")]
y <- data$outcome

max_depth <- 2

tree <- build_regression_tree(X, y, max_depth)

# Stampa dell'albero di regressione
print(tree)


#Questa implementazione crea un albero di regressione utilizzando l'algoritmo CART. Viene utilizzata una funzione di costo basata sull'errore quadratico medio (mean squared error) per valutare la qualità delle suddivisioni. L'albero viene costruito ricorsivamente dividendo i dati in base alle feature e ai valori di soglia che minimizzano il costo.
```

## Model Setup
Bootstrap aggregating Regression Trees can be done following the procedure proposed by `r Cite(bibliography, key = "Breiman")`:

visuals bagging application - model setting (paragrafo 5+ fig1)

# Data Exploratory Analysis (DEA)

This data can be used to compare with cancer genes and label these data
to predict and diagnose cancers such as breast cancer and also to
diagnose the stage of different cancers. Also, from the expression of
these DNAs, artificial intelligence models can be obtained that can
predict future mutations in any disease, or find the mechanisms of
protein production and their gene ontology, and treat, diagnose and
predict the disease such as cancer.

```{r eval=FALSE, include=FALSE}
df = 
  

  

ames_bag1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)


ames_bag2 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 10),
  nbagg = 200,  
  control = rpart.control(minsplit = 2, cp = 0)
)


```

```{r eval=FALSE, include=FALSE}
# Application
 # Load the randomForest package

# Assuming your dataset is named "your_dataset" and the target variable is "Ozone"

# Set the number of trees in the ensemble
num_trees <- 150

# Set the control parameters for the CART algorithm
cart_control <- rpart.control(minsplit = 2, cp = 0)

# Apply bagging using randomForest
bagged_model <- randomForest(Ozone ~ ., data = air, ntree = num_trees, control = cart_control)

# Visualize an individual tree from the bagged ensemble
individual_tree <- bagged_model$forest[[1]]  # Select the first tree (you can choose any tree from the ensemble)

rpart.plot(individual_tree)
```

Try to replicate the algorithm provided by the paper for regression trees using 10-fold cross-validation and bagging

```{r eval=FALSE, include=FALSE}
# Perform cross-validation
k_fold_reg_tree= function(df,k, y){
  formula <- as.formula(paste(y, "~", "."))
  # 1. take the dataset and divide it into learning and testing (90% - 10%)
  fold_indices <- sample(cut(seq(1, nrow(df)), breaks = k, labels = FALSE, ordered_result = FALSE))
  test_indices <- which(fold_indices == i)
  train_i <- df[-test_indices, ]
  test_i <- df[test_indices, ]
  
  
  # 2. take the learning set and apply 10-fold cv to find the model with lowest MSE
  
  # randomly chose the division of learning and testing as pointed out in fifth point of the algorithm by BBEIMAN
  fold_indices <- sample(cut(seq(1, nrow(train_i)), breaks = k, labels = FALSE, ordered_result = FALSE))
  
  cv_result=100000000000000000000000000
  
  for (i in 1:k) {

  
  # Split the data into training and testing sets
  test_indices <- which(fold_indices == i)
  train_data <- train_i[-test_indices, ]
  test_data <- train_i[test_indices, ]
  
  
  
  # Train a regression tree on the training set
  model <- rpart(formula, data = train_data)
  
  # Predict the target variable for the testing set
  predictions <- predict(model, newdata = test_data)
   
  
  #Calculate the evaluation metric (e.g., mean squared errors
  result <- mean((predictions - test_data[[y]])^2)
  # 3. For every candidate we compute the MSE compered to the original test
  
  if(result<cv_result){
    cv_result=result
    # Compare this model with the original test set
    predictions <- predict(model, newdata = test_i)
    final_res=mean((predictions - test_i[[y]])^2)
  }
  }
  return(final_res)
}

reg_tree_boot= function(df,k, y){
  formula <- as.formula(paste(y, "~", "."))
  
  # 1. take the dataset and divide it into learning and testing (90% - 10%)
  fold_indices <- sample(cut(seq(1, nrow(df)), breaks = k, labels = FALSE, ordered_result = FALSE))
  test_indices <- which(fold_indices == 10)
  train_i <- df[-test_indices, ]
  test_i <- df[test_indices, ]
  
  predictions=0
  
  # 2. compute 25 predictions with different bootstrap samples
  for (i in 1:k) {
  train_data=slice_sample(train_i,n=nrow(train_i), replace=TRUE)
  model <- rpart(formula, data = train_data)
  
  
  # Predict the target variable for the testing set
  predictions <- predictions + predict(model, newdata = test_i)
   
  }
  # 3. take the mean of these predictors and compute the MSE
  predictions=predictions/k
  final_res=mean((predictions - test_i[[y]])^2)
  return(final_res)
}

housing <- read.table("housing.csv", quote="\"", comment.char="")
colnames(housing)=c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',
                    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV')

ind=sample(1:nrow(housing), nrow(housing))
nrow(housing)

train= housing[ind[1:as.integer(nrow(housing)*0.90)],]

test= housing[ind[(as.integer(nrow(housing)*0.90)+1):nrow(housing)],]
 
# Now that we have a function that takes the dataset as input, performs 10-fold cv
# on the learning set, selects the best fold and use it to make predictions.
# Only then we can compute the MSE that we are going to use
e_S_bar=mean(replicate(1000,k_fold_reg_tree(housing,10, "MEDV")))
e_B_bar=mean(replicate(1000,reg_tree_boot(housing, 25, "MEDV")))

((e_S_bar-e_B_bar)/e_S_bar)*100 # increase

```

Do the same but this time we use libraries that simplifies the coding part a lot

```{r eval=FALSE, include=FALSE}
# Set the number of folds
k <- 10
nbagg <- 25
# Create a control object for 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = k)

# Train the regression tree using 10-fold cross-validation
e_S_bar <- mean(replicate(1000,train(MEDV ~ ., data = housing, method = "rpart", trControl = ctrl)$results$RMSE[1]^2))
e_S_bar=mean(model)
# Train the bagging model
e_B_bar <- mean(replicate(1000,train(MEDV ~ ., data = housing, method = "treebag", nbagg = nbagg)$results$RMSE[1]^2))

((e_S_bar-e_B_bar)/e_S_bar)*100 # increase

```


# Application

```{r eval=FALSE, include=FALSE}
# Caricamento del dataset
# data = 

# Divisione del dataset in set di addestramento e set di test
set.seed(42)
train_indices <- sample(1:nrow(data), 0.7 * nrow(data)) # sistemare quanti dati
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

```


```{r eval=FALSE, include=FALSE}
# Creazione dataset di addestramento -> bootstrap
n_bootstraps <- 10 # sistemare quanti bootstrap
train_datasets <- lapply(1:n_bootstraps, function(i) train_data[sample(1:nrow(train_data), replace = TRUE), ])

```


```{r eval=FALSE, include=FALSE}
# Modelli di regressione lineare su ciascun dataset di addestramento
# models <- map(train_datasets, function(data) lm( ~ ., data = data)) #inserire le variabili

# Calcolo della media dei beta
coefficients <- lapply(models, coef)
average_coefficients <- colMeans(do.call(rbind, coefficients))

# final_model <- lm( ~ ., data = train_data)
ols = final_model
summary(ols)
final_model$coefficients <- average_coefficients


```


```{r eval=FALSE, include=FALSE}
# Valutazione del modello 
pred_train = predict(final_model, newdata = train_data)
# r_squared <- cor(test_data$, pred_train)^2 
p <- length(final_model$coefficients) - 1
# n <- length(train_data$) # aggiungi la y
r_squared_adjusted <- 1 - (1 - r_squared) * (n - 1) / (n - p - 1)
f_statistic <- (r_squared / p) / ((1 - r_squared) / (n - p - 1))
p_value <- pf(f_statistic, p, n - p - 1, lower.tail = FALSE)
# residuals <- train_data$ - predictions
std_error <- sqrt(sum(residuals^2) / (length(residuals) - length(final_model$coefficients)))


```

```{r eval=FALSE, include=FALSE}
# Valutazione delle prestazioni
predictions <- predict(final_model, newdata = test_data)
# r_squared <- cor(test_data$, predictions)^2           
# n <- length(test_data$) # aggiungi la y                            
p <- length(final_model$coefficients) - 1
r_squared_adjusted <- 1 - (1 - r_squared) * (n - 1) / (n - p - 1)
f_statistic <- (r_squared / p) / ((1 - r_squared) / (n - p - 1))
p_value <- pf(f_statistic, p, n - p - 1, lower.tail = FALSE)
# residuals <- test_data$ - predictions
std_error <- sqrt(sum(residuals^2) / (length(residuals) - length(final_model$coefficients)))

```





