---
title: "Final Project: Bagging Predictors"
author: "Manuel Bottino, Patrick Poetto, Jacopo Spagliardi"
date: "`r Sys.Date()`"

output:
  pdf_document: 
    toc: true
    number_sections: true
    latex_engine: pdflatex
fig_caption: yes
urlcolor: blue
header-includes:
  - \usepackage{amssymb}
editor_options: 
  markdown: 
    wrap: 72
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(magrittr)     #Pipe operator for easier data manipulation.
library(dplyr)        #Data manipulation and transformation.
library(ggplot2)      #Data visualization using the grammar of graphics.
library(gridExtra)    #Arrange multiple plots on a grid.
library(ggpubr)       #Extension of ggplot2 for publication-ready graphics.
library(bookdown)     #Authoring books and documents with R Markdown.
library(plotly)       #Interactive and web-based visualizations.
library(rstanarm)     #Bayesian applied regression modeling.
library(Cairo)        #Graphics device for creating high-quality images.
library(rgl)          #3D visualization using OpenGL.
library(plot3D)       #3D scatterplots and surface plots.
library(effects)      # effects - Effect displays for statistical models.
library(MASS)         #Functions and datasets for applied statistics.
library(car)          #Companion to Applied Regression textbook.
library(faraway)      #Functions and datasets for regression and ANOVA.
library(leaps)        #Feature selection and regression subset selection.
library(ROCR)         #Performance evaluation of binary classifiers.
library(RColorBrewer) #Color palettes for data visualization.
library(ISLR2)        #Data and functions for the book "A.I.T.S.L.W.A.I.R" 
library(e1071)        #Various functions for statistical learning/classification.
library(class)        #Functions for classification.
library(reshape2)     #Reshape and manipulate data.
library(sads)         #Species abundance distributions.
library(knitr)        #Dynamic report generation with R Markdown.
library(latex2exp)    #LaTeX expressions in plots.
library(GGally)       #Extension to ggplot2 for matrix-based plots.
library(combinat)     #Permutations, combinations, and combinatorial datasets.
library(superheat)    #Heatmaps with improved aesthetics and interactivity.
library(stringr)      #String manipulation functions.
library(purrr)        #Functional programming with vectors and lists.
library(tidyr)        #Tidy data principles for data cleaning.
library(lubridate)    #Easy handling of dates and times.
library(ggthemes)     #Additional themes for ggplot2.
library(kableExtra)   #Create complex tables using 'kable' and 'knitr'.
library(covr)         #Test coverage reports for R packages.
library(doParallel)   #Parallel computing using the foreach framework.
library(foreach)      #Looping construct for iterating over elements.
library(caret)        #Classification and regression training package.
library(rpart)        #Recursive partitioning for decision trees.
library(ipred)        #Improved predictive models.
library(rpart.plot)   #Plotting decision trees from rpart objects.
library(tidymodels)   #Tidyverse-style modeling framework.
library(tidyr)        #Tidy data principles for data cleaning.
library(methods)      #Statistical and analytical methods for data analysis.
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Review

## Why it works
First of all let's introduce the general idea of bagging. Given a dataset $L=\{(x_n,y_n),\ n=1, \ldots ,N\}$ we try to improve a predicting procedure in a "naive" way, ideally we would like to have a sequence of datasets $\{L_k\}$ of training sets each containing  N independent observations, then take the average of the sequence $\{\phi(x, L_k)\}$.

\begin{equation}
\phi_A(x,P)=E_L[\phi(x,L)]\label{eq:agg}
\end{equation}.

Because $\phi_A$ depends not only on x but also the underlying probability distribution $P$ from which L is drawn. Since we are missing this last information the only tool we are left with is our dataset L, we instead use $P_L$ the bootstrap approximation of $P$, it can be defined as the probability mass function with equal probability of extracting each element of the dataset $(x_n,y_n)$. Finally we can define the bootstrap aggregate predictor as:

\begin{equation}
\phi_B (x)=\phi_A(x,P_L)\label{eq:boot}
\end{equation}.

In order to understand why bagging works theoretically it can be proved that mean-squared error (MSE) of $\phi_A(x)$ is lower than the mean-squared error averaged over L of $\phi(x,L)$. How much lower the two sides are depends on the inequality: 

\begin{equation}
[E_L\phi(x,L)]^2\leq E_L\phi^2(x,L)\label{eq:ineq}
\end{equation}

This result is true taking advantage of the Jensen’s inequality for the specific case in which $g(X)=X^2$, this function is convex ($g''>0$), thus $E[Z^2] \geq (E[Z])^2$


## Instability
The inequality is a nice starting point to explain what role instability plays. In fact, If $\phi(x, L)$ does not change too much with replicate L the two sides will be nearly equal, and aggregation will not help. The more highly variable the $\phi(x, L)$ are, the more improvement aggregation may produce. Applying this reasoning to our  $\phi_B (x)$, if the procedure is unstable, it can give improvement through aggregation. However, if the procedure is stable, then $\phi_B (x)=\phi_A(x,P_L)$ will not be as accurate for data drawn from $P$ as $\phi_A(X, P) \sim \phi(x,L)$.

Let's see how this concept can be translated when we look at a more specific context, like classification. In this instance the predictor $\phi(\pmb x,L)$ predicts a label $j \in \{1,..., J\}$. We first define $Q(j|x)=P(\phi(x,L)=j)$, over many independent replicates of the learning set $L$, $\phi$ predicts class label $j$ at input $x$ with relative frequency $Q(j|x)$, and let $P(j|x)$ be the probability that input x generates class $j$.
At this point we can set $\phi^*(x)= \arg \max _j P(j|x)$ (the Bayes predictor) which leads to the following expression for $Q (j |x)$:

\[
\begin{cases}
1 \ if \ P(j|x)=max_iP(i|x) \\
0 \ elsewhere
\end{cases}
\]

Now we have all the ingredients to show the maximum classification rate where:
\begin{equation}
r^*=\int \max_j P(j|x) P_X(dx)\label{eq:class_max}
\end{equation}

where $P_X(dx)$ is the probability distribution of X.

If we know focus on the aggregate of $\phi$ and define it following the procedure described above $\phi_A(x)=\arg \max_j Q(j|x)$. We have the maximum attainable correct-classification rate of $\phi(x)$, we are missing the corret-classification for x, which for $\phi_A(x)$ is $\sum_j I(\arg \max_j Q(j|x)=j)P(j|x)$ Where $I$ is the indicator function. 
*Putting all the pieces together* the correct-classification rate for $\phi_A(x)$ is:
\begin{equation}
r_A=\int_{\pmb x \in C} max_j P(j| \pmb x) P_X(d \pmb x)+ \int_{\pmb x \in C’} [\sum_j(I(\phi_A(\pmb x)=j)P(j|\pmb x)] P_X(\pmb x)
\label{eq:class_res}
\end{equation}
Even if $\phi$ is order correct at $x$ its correct classification rate can be far from optimal, but $\phi_A$ is optimal. Only if $\phi$ is order-correct for most input of x (it is good), then aggregation can transform it into a nearly optimal predictor. On the other hand, unlike the numerical prediction situation, poor predictors can be transformed into worse ones if we use bagging.

## Model - Decision Tree-Based Methods

Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model, say a constant, in each one. This is a simple yet powerful procedure since it is able to disentangle a model into simpler and smaller models and describe his features more accurately than global models do basically using binary conditional clustering. The geometric perspective described before can be seen as a tree where data are run and at each node a test is conducted to see what is the path a covariate should follow until reaching a leaf, which represents the final prediction explained by the constant model. 
For example, let's say we have $p$ inputs and a response, for each of $N$ observations: that is, $(x_i,y_i)$ for $i = 1,2,...,N$, with $x_i = (x_{i1},x_{i2},...,x_{ip})$. The algorithm has to decide on the splitting variables and split points, as well as what shape the tree
should have. Suppose first that we have a partition into $M$ regions $R_1, R_2, ..., R_M$, and we model the response as a constant $c_m$ in each region: As a criterion for optimal partitional we can minimize the sum of squares $\sum(y_i-f(x_i))^2$. In this way the best $\hat{c}_m$ is just the average of $y_i$ in region $R_m$: $\hat{c}_m=av(y_i | x_i \in R_m)$. Our model will be then: $f(x)=\sum_{m=1}^{M} c_m \cdot I(x \in R_m)$.

Now finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible so we can set up a CART (classification and regression tree) algorithm starting with the data, a splitting variable $j$, a split point $s$, and defining the half planes as: $R_1(j,s) = \{X\mid X_j \leq s\}$ and $R_2(j,s) = \{X\mid X_j > s\}$. Then we seek $j$ and $s$ that solve 

```{=latex}
\begin{center}
$\underset{j,s}{\min}\left[\underset{c_1}{\min}\sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \underset{c_2}{\min} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2\right]$
\end{center}
```

For any choice $j$ and $s$, the inner minimization is solved by: 

```{=latex}
\begin{center}
$\hat{c}_1 = \text{av}(y_i \,|\, x_i \in R_1(j, s)) \quad \text{and} \quad \hat{c}_2 = \text{av}(y_i \,|\, x_i \in R_2(j, s))$
\end{center}
```

For each $j$, the split point $s$ can be found very quickly and hence determination of the best pair (j, s) is feasible by brute force. Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. Then this process is repeated on all of the resulting regions.
The question now become: how large should we grow the three? It is pretty straightforward that too many nodes (splits) may overfit the data while doing vice versa may end up not being able to capture them well, resulting in misprediction. One strategy could be to set a lower threshold for the decrease of the sum of squares and stop the splitting when this is reached. However this strategy is too short sighted since a seemingly worthless split may lead to a very good one below. A more robust strategy may be to do kind of the opposite: grow a very large tree and then use a cost-complexity pruning criterion to collapse one internal node at a time from the full tree until the single node tree so that we find a sequence. It is intuitive that the optimal tree must be somewhere in the sequence. Now, with a cross validation selection method, we can find the actual optimal tree just minimizing the cross validated sum of squares. 

This is how the CART algorithm for growing decision trees basically works. Note that decision trees are divided in classification and regression trees if the response is a factor or a numerical or continous variable. 

```{r eval=FALSE, include=FALSE}
#Below it is proposed a very basic implementation of a CART algorithm using mean squared error cost complexity criterion. The algorithm is not implemented to prune the tree. 

#CART Algorithm implementation 
#Defining the cost-complexity function for splitting
calculate_cost <- function(y) {
  if (any(is.na(y))) {
    return(Inf)
  }
  
  n <- length(y)
  mean_squared_error <- sum((y - mean(y))^2) / n
  return(mean_squared_error)
}

#Defining optimal splitting research function 
find_best_split <- function(X, y) {
  best_cost <- Inf
  best_feature <- NULL
  best_threshold <- NULL

  for (i in 1:ncol(X)) {
    feature_values <- unique(X[, i])
    for (value in feature_values) {
      left_indices <- X[, i] <= value
      right_indices <- X[, i] > value

      left_y <- y[left_indices]
      right_y <- y[right_indices]

      cost <- calculate_cost(left_y) + calculate_cost(right_y)

      if (cost < best_cost) {
        best_cost <- cost
        best_feature <- i
        best_threshold <- value
      }
    }
  }

  return(list(feature = best_feature, threshold = best_threshold))
}

# Defining regression tree class 
RegressionTree <- setRefClass("RegressionTree",
  fields = list(
    feature = "numeric",
    threshold = "numeric",
    value = "numeric",
    left_child = "RegressionTree",
    right_child = "RegressionTree"
  )
)

# Defining regression tree building fucntion 
build_regression_tree <- function(X, y, max_depth) {
  if (length(unique(y)) == 1 || max_depth == 0) {
    leaf <- RegressionTree()
    leaf$value <- mean(y)
    return(leaf)
  }

  split <- find_best_split(X, y)
  feature <- split$feature
  threshold <- split$threshold

  left_indices <- X[, feature] <= threshold
  right_indices <- X[, feature] > threshold

  left_X <- X[left_indices, ]
  left_y <- y[left_indices]

  right_X <- X[right_indices, ]
  right_y <- y[right_indices]

  node <- RegressionTree()
  node$feature <- feature
  node$threshold <- threshold
  node$left_child <- build_regression_tree(left_X, left_y, max_depth - 1)
  node$right_child <- build_regression_tree(right_X, right_y, max_depth - 1)

  return(node)
}

#Regression tree algorithm 
max_depth <- 6

tree <- build_regression_tree(X = data[, -8], y = data$pop, max_depth)
```

```{r include=FALSE}
data = readRDS("/Users/pat/Documents/GitHub/Computational-stat/Final project/spot.RDS")
y = data$pop
rem = c(5, 6, 7, 8, 10, 11, 12, 16, 17, 18)
data = data[, -rem]

#New Observation
new_data <- data.frame(
  acousticness = median(data$acousticness),
  danceability = median(data$danceability),  
  duration_ms = median(data$duration_ms),
  energy = median(data$energy),
  loudness = median(data$loudness),
  speechiness = median(data$speechiness),
  tempo = median(data$tempo),
  valence = median(data$valence)
)
```

```{r echo=FALSE, fig.height=4.2, fig.width=8}
#Using function rpart to have a plottable object
fit <- rpart(y ~ ., data = data, maxdepth = 8)
rpart.plot(fit, tweak=0.8)
color_scale <- c("blue", "white") 
legend(x=0.9, y=0.85, legend = c("High", "Low"), fill = color_scale, 
       title = "Homogeneity", cex=0.6) 


#Making prediction
predictions <- predict(fit, newdata = new_data)
```

| Predicted Value | Median Value |
|-----------------|--------------|
| `r predictions` | `r median(y)`|

We used the well known Spotify dataset to grow a basic tree with the function "rpart". The thresholds at each node are defined by the algorithm which minimizes the MSE of the model. The color of each nodes represent homogeneity across data which have been classified in a node. Each node show the percentage of observations that it contains and the outcome prediction for that node. 
Then we used the function "predict" applied to our grown tree to make prediction on a virtual new observation, providing the median of each variable as a new value and getting predicted popularity of a song which have the median as each feature available in the dataset. Not surprisingly, the predicted value for popularity is near the median of the covariate popularity itself. 


## Model setup 

Bootstrap aggregating Regression Trees can be done following the procedure proposed by Breiman: 

  1. Randomly divide a real dataset into a 10% test $\mathcal\mathbf({T})$ and a 90% learning set $\mathcal\mathbf({L})$. If the dataset is simulated then we can consider a 15-85 or a 20-80 proportion. 
  
  2. Grow a regression tree from $\mathcal\mathbf({L})$ using a 10-fold cross validation, then run $\mathcal\mathbf({T})$ down the tree to obtain the squared error $e_S(\mathcal{L}, \mathcal{T})$. Using che k-folds cross validation means that $\mathcal\mathbf({L})$ is divided into 10 folds, and for each iteration, a regression tree is trained on 9 folds and evaluated on the remaining fold. The process is repeated 10 times, such that each fold is utilized as the test set once. Ultimately, the best model, i.e., the regression tree with the best average performance across all test folds, is selected as the final result. At the end of the 10 iterations, the squared errors obtained for each test fold are used to calculate the mean squared error $e_S(\mathcal{L}, \mathcal{T})$, which represents the average squared difference between the predictions of the regression tree and the actual values of the test set.
  
  3. Select a bootstrap sample $\mathcal\mathbf({L})_B$ from $\mathcal\mathbf({L})$ and use it to grow a tree. Then use $\mathcal\mathbf({L})$ to prune the tree avoiding overfitting. Repeat this step 25 times to obtain predictors $\phi_1(\mathbf{x}), ..., \phi_{25}(\mathbf{x})$.
  
  4. For $(y_n, \mathbf(x_n)) \in \mathcal\mathbf({T}))$, the bagged predictor will be $\hat{y}_n = av_k\phi_k(\mathbf{x_n})$, and the squared error $e_B(\mathcal{L}, \mathcal{T}) = av_n(y_n-\hat{y}_n)^2$.
  
  5. Divide the data into $\mathcal{L}$ and $\mathcal{T}$ for 100 times and average the errors to obtain $\overline{e}_S$ and $\overline{e}_B$.


# Data Exploratory Analysis (DEA)

This data can be used to compare with cancer genes and label these data to predict and diagnose cancers such as breast cancer and also to diagnose the stage of different cancers. Also, from the expression of these DNAs, artificial intelligence models can be obtained that can predict future mutations in any disease, or find the mechanisms of protein production and their gene ontology, and treat, diagnose and predict the disease such as cancer.

```{r eval=FALSE, include=FALSE}
df = 
  

  

ames_bag1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)


ames_bag2 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 10),
  nbagg = 200,  
  control = rpart.control(minsplit = 2, cp = 0)
)


```



# References

knitr::write_bib()
