---
title: "Final_project_Bottino_Poetto_Spagliardi"
author: "Manuel Bottino, Patrick Poetto, Jacopo Spagliardi"
date: "`r Sys.Date()`"

output:
  pdf_document: 
    toc: true
    number_sections: true
    latex_engine: pdflatex
fig_caption: yes
urlcolor: blue
header-includes:
  - \usepackage{amssymb}
editor_options: 
  markdown: 
    wrap: 72
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(magrittr)     #Pipe operator for easier data manipulation.
library(dplyr)        #Data manipulation and transformation.
library(ggplot2)      #Data visualization using the grammar of graphics.
library(gridExtra)    #Arrange multiple plots on a grid.
library(ggpubr)       #Extension of ggplot2 for publication-ready graphics.
library(bookdown)     #Authoring books and documents with R Markdown.
library(plotly)       #Interactive and web-based visualizations.
library(rstanarm)     #Bayesian applied regression modeling.
library(Cairo)        #Graphics device for creating high-quality images.
library(rgl)          #3D visualization using OpenGL.
library(plot3D)       #3D scatterplots and surface plots.
library(effects)      # effects - Effect displays for statistical models.
library(MASS)         #Functions and datasets for applied statistics.
library(car)          #Companion to Applied Regression textbook.
library(faraway)      #Functions and datasets for regression and ANOVA.
library(leaps)        #Feature selection and regression subset selection.
library(ROCR)         #Performance evaluation of binary classifiers.
library(RColorBrewer) #Color palettes for data visualization.
library(ISLR2)        #Data and functions for the book "A.I.T.S.L.W.A.I.R" 
library(e1071)        #Various functions for statistical learning/classification.
library(class)        #Functions for classification.
library(reshape2)     #Reshape and manipulate data.
library(sads)         #Species abundance distributions.
library(knitr)        #Dynamic report generation with R Markdown.
library(latex2exp)    #LaTeX expressions in plots.
library(GGally)       #Extension to ggplot2 for matrix-based plots.
library(combinat)     #Permutations, combinations, and combinatorial datasets.
library(superheat)    #Heatmaps with improved aesthetics and interactivity.
library(stringr)      #String manipulation functions.
library(purrr)        #Functional programming with vectors and lists.
library(tidyr)        #Tidy data principles for data cleaning.
library(lubridate)    #Easy handling of dates and times.
library(ggthemes)     #Additional themes for ggplot2.
library(kableExtra)   #Create complex tables using 'kable' and 'knitr'.
library(covr)         #Test coverage reports for R packages.
library(doParallel)   #Parallel computing using the foreach framework.
library(foreach)      #Looping construct for iterating over elements.
library(caret)        #Classification and regression training package.
library(rpart)        #Recursive partitioning for decision trees.
library(ipred)        #Improved predictive models.
library(rpart.plot)   #Plotting decision trees from rpart objects.
library(tidymodels)   #Tidyverse-style modeling framework.
library(tidyr)        #Tidy data principles for data cleaning.
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Review

### Why it works

### Stability

### Model - Decision Tree-Based Methods

Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model, say a constant, in each one. This is a simple yet powerful procedure since it is able to disentangle a model into simpler and smaller models and describe his features more accurately than global models do basically using binary conditional clustering. The geometric perspective described before can be seen as a tree where data are run and at each node a test is conducted to see what is the path a covariate should follow until reaching a leaf, which represents the final prediction explained by the constant model. 
For example, let's say we have $p$ inputs and a response, for each of $N$ observations: that is, $(x_i,y_i)$ for $i = 1,2,...,N$, with $x_i = (x_{i1},x_{i2},...,x_{ip})$. The algorithm has decide on the splitting variables and split points, as well as what shape the tree
should have. Suppose first that we have a partition into $M$ regions $R_1, R_2, ..., R_M$, and we model the response as a constant $c_m$ in each region: As a criterion for optimal partitional we can minimize the sum of squares $\sum(y_i-f(x_i))^2$. In this way the best $\hat{c}_m$ is just the average of $y_i$ in region $R_m$: $\hat{c}_m=av(y_i | x_i \in R_m)$. Our model will be then: $f(x)=\sum_{m=1}^{M} c_m \cdot I(x \in R_m)$.

Now finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible so we can set up a CART (classification and regression tree) algorithm starting with the data, a splitting variable $j$, a split point $s$, and defining the half planes as: $R_1(j,s) = \{X\mid X_j \leq s\}$ and $R_2(j,s) = \{X\mid X_j > s\}$. Then we seek $j$ and $s$ that solve 

```{=latex}
\begin{center}
$\underset{j,s}{\min}\left[\underset{c_1}{\min}\sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \underset{c_2}{\min} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2\right]$
\end{center}
```

For any choice $j$ and $s$, the inner minimization is solved by: 

```{=latex}
\begin{center}
$\hat{c}_1 = \text{av}(y_i \,|\, x_i \in R_1(j, s)) \quad \text{and} \quad \hat{c}_2 = \text{av}(y_i \,|\, x_i \in R_2(j, s))$
\end{center}
```

For each $j$, the split point $s$ can be found very quickly and hence determination of the best pair (j, s) is feasible by brute force. Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. Then this process is repeated on all of the resulting regions.
The question now become: how large should we grow the three? It is pretty straightforward that too many nodes (splits) may overfit the data while doing vice versa may end up not being able to capture them well, resulting in misprediction. One strategy could be to set a lower threshold for the decrease of the sum of squares and stop the splitting when this is reached. However this strategy is too short sighted since a seemingly worthless split may lead to a very good one below. A more robust strategy may be to do kind of the opposite: grow a very large tree and then use a cost-complexity pruning criterion to collapse one internal node at a time from the full tree until the single node tree so that we find a sequence. It is intuitive that the optimal tree must be somewhere in the sequence. Now, with a cross validation selection method, we can find the actual optimal tree just minimizing the cross validated sum of squares. 

This is how the CART algorithm for growing decision trees basically works. Note that decision trees are divided in classification and regression trees if the response is a factor or a numerical or continous variable. 
Below it is proposed a very basic implementation of a CART algorithm using mean squared error cost complexity criterion. The algorithm is not implemented to prune the tree. 

```{r eval=FALSE, include=FALSE}
#implementazione CART
# Definizione della funzione di costo per la suddivisione
calculate_cost <- function(y) {
  n <- length(y)
  mean_squared_error <- sum((y - mean(y))^2) / n
  return(mean_squared_error)
}

# Definizione della funzione per la ricerca della miglior suddivisione
find_best_split <- function(X, y) {
  best_cost <- Inf
  best_feature <- NULL
  best_threshold <- NULL

  for (i in 1:ncol(X)) {
    feature_values <- unique(X[, i])
    for (value in feature_values) {
      left_indices <- X[, i] <= value
      right_indices <- X[, i] > value

      left_y <- y[left_indices]
      right_y <- y[right_indices]

      cost <- calculate_cost(left_y) + calculate_cost(right_y)

      if (cost < best_cost) {
        best_cost <- cost
        best_feature <- i
        best_threshold <- value
      }
    }
  }

  return(list(feature = best_feature, threshold = best_threshold))
}

# Definizione della classe dell'albero di regressione
class RegressionTree {
  feature <- NULL
  threshold <- NULL
  value <- NULL
  left_child <- NULL
  right_child <- NULL
}

# Definizione della funzione per la costruzione dell'albero di regressione
build_regression_tree <- function(X, y, max_depth) {
  if (length(unique(y)) == 1 || max_depth == 0) {
    leaf <- RegressionTree()
    leaf$value <- mean(y)
    return(leaf)
  }

  split <- find_best_split(X, y)
  feature <- split$feature
  threshold <- split$threshold

  left_indices <- X[, feature] <= threshold
  right_indices <- X[, feature] > threshold

  left_X <- X[left_indices, ]
  left_y <- y[left_indices]

  right_X <- X[right_indices, ]
  right_y <- y[right_indices]

  node <- RegressionTree()
  node$feature <- feature
  node$threshold <- threshold
  node$left_child <- build_regression_tree(left_X, left_y, max_depth - 1)
  node$right_child <- build_regression_tree(right_X, right_y, max_depth - 1)

  return(node)
}

# Utilizzo dell'algoritmo CART per costruire un albero di regressione
data <- data.frame(
  age = c(25, 30, 35, 40),
  income = c(50000, 60000, 70000, 80000),
  outcome = c(100, 150, 200, 250)
)

X <- data[, c("age", "income")]
y <- data$outcome

max_depth <- 2

tree <- build_regression_tree(X, y, max_depth)

# Stampa dell'albero di regressione
print(tree)


#Questa implementazione crea un albero di regressione utilizzando l'algoritmo CART. Viene utilizzata una funzione di costo basata sull'errore quadratico medio (mean squared error) per valutare la qualitÃ  delle suddivisioni. L'albero viene costruito ricorsivamente dividendo i dati in base alle feature e ai valori di soglia che minimizzano il costo.
```

visuals bagging application - model setting (paragrafo 5+ fig1)

# Data Exploratory Analysis (DEA)

This data can be used to compare with cancer genes and label these data
to predict and diagnose cancers such as breast cancer and also to
diagnose the stage of different cancers. Also, from the expression of
these DNAs, artificial intelligence models can be obtained that can
predict future mutations in any disease, or find the mechanisms of
protein production and their gene ontology, and treat, diagnose and
predict the disease such as cancer.

```{r eval=FALSE, include=FALSE}
df = 
  

  

ames_bag1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)


ames_bag2 <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 10),
  nbagg = 200,  
  control = rpart.control(minsplit = 2, cp = 0)
)


```

```{r eval=FALSE, include=FALSE}
####### a caso 


# Crea un set di dati di esempio per la regressione
data <- data.frame(
  age = c(25, 30, 35, 40),
  income = c(50000, 60000, 70000, 80000),
  outcome = c(100, 150, 200, 250)
)

# Crea un albero di regressione
tree <- rpart(outcome ~ age + income, data = data)

# Stampa l'albero di regressione
print(tree)
```

# References

knitr::write_bib()
