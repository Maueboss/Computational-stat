---
title: "Assignment I - CompStat2023"
author: "Name of the Team"
date: "Team Member A, Team Member B, Team Member C"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: readable
    toc: yes
    code_folding: show
    df_print: paged
---

# Exercise 4: MC integration
Consider a standard Normal random variable X: 
\begin{equation}
X\sim \mathcal{N}(\mu = 0, \sigma^2 = 1)    
\end{equation}

## Point a)

If we want to calculate $\mathbb{P}(X>20)$ we could try to integrate the PDF of X in this way:
\begin{equation}
    \mathbb{P}(X>20) = 1 - \Phi(20) = 1 - \int_{-\infty}^{20} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx = \int_{20}^{+\infty}  \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx
\end{equation}

We immediately see that the integrand function is not so easy to be integrated (it is the son of the Gaussian function $e^{x^2}$). We will try to estimate this with the Monte Carlo method but, although this method is extremely powerful and flexible, there are some situations in which it can fail.

The reason why the Monte Carlo crude method fail in this case, is because the Monte Carlo Method requires a finite interval where the uniform distribution used by this method can generate random samples. In this particular case of the probability density function of a standard normal distribution the support is infinity so we have not got the extreme value where we can generate value. A possible solution will be the change of variable (see point b)

Additionally, this function does not have an analytical solution and therefore integration would require the use of numerical techniques that would further increase computational complexity.

## Point b)
Considering the change of variable $Y = \frac{1}{X}$ the integral will become:
\begin{equation}
    \mathbb{P}(X>20) = \mathbb{P}(Y<1/20) = \int_{0}^{1/20}  \frac{1}{\sqrt{2\pi}*y^2} e^{-\frac{(1/y)^2}{2}} dy
\end{equation}
because $f_Y(y) = f_X(g(y)) |\frac{\delta g(y)}{\delta y}|$ where:
\begin{itemize}
    \item $g(y) = \frac{1}{y}$
    \item $\frac{\delta g(y)}{\delta y}= -\frac{1}{y^2}$
    \item $f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$
\end{itemize}
As the first one, this integral has not have an analytic solution so we use the Monte Carlo method to estimate. The idea is to generate N random numbers from a uniform distribution between 0 and 1/20 that we obtain from the variable change. We can do this because now we have finite support where we can generate value.

\begin{equation}
    \mathbb{P}(X>20) \approx \frac{0.05 - 0}{N} \sum_{i=1}^{N} \frac{1}{\sqrt{2\pi}y_i^2} \cdot e^{-\frac{(1/y_i)^2}{2}}
\end{equation}

```{r}
rm(list = ls())
set.seed(1234)

# Define the PDF of N(0,1)
f = function(x) 1/(sqrt(2*pi)*x^2)*exp(-(1/x)^2/2)

# Estimate the sd with a single run
cumsd_f = function(x){
  
  sapply(2:length(x), function(r) sd(f(x[1:r])))
  
}


# Plot the ergodic function and its uncertainty
plot_ergodic = function(output,theta){
  
  A = output
  x_top =    max(A[,1]+ 1.96 * A[,2])
  x_bottom = min(A[,1]- 1.96 * A[,2])
  
  plot(A[,1],type="l",col=4,lwd=2,ylim = c(x_bottom,x_top))
  lines(A[,1] + 1.96 * A[,2],
        type="l",col="darkblue",lwd=2)
  lines(A[,1] - 1.96 * A[,2],
        type="l",col="darkblue",lwd=2)
  
  abline(h=theta,col=2)
  
}

MC_ergodic = function(n_max, a=0, b=1/20){
  
  x = runif(n_max,a,b)
  
  thetahat = (b-a) * cumsum(f(x))/c(1:n_max)
  sdthetahat = (b-a)/sqrt(2:n_max) * cumsd_f(x)
  
  return(cbind(thetah = thetahat[-1], sd = sdthetahat))
}

# use the output of the integrate function as the real value
theta = integrate(f,0,1/20)
n = 10000
MC_res = MC_ergodic(n)
plot_ergodic(MC_res, theta[1])
```

The ergodic plot a tool used to assess the convergence of results obtained. Ergodicity is an important property of the Monte Carlo method, which states that the system reaches a statistically representative equilibrium state after a sufficient number of iterations and visualize the convergence of results to the expected value or correct solution. In this case we can see in light blue the estimation of the integral in base of the number of value generate and in dark blue the confidence interval of level 95\%. Compare to the red line, that represent the \textit{real} value given by the \texttt{integrate} function of \texttt{R}, we can see that this value isn't in the confidence interval so we have to implement our method. 

## Point c) and d)
Now we try to construct a more efficient estimator using antithetic variables it will be more efficient because it reduces the variance of the estimator because we are using 2 unbiased estimators identically distributed but negatively correlated and for definition the variance decreases. 

```{r}
# Repeat everything but using antithetic variables
cumsd_AV = function(x,xprime){
  sapply(2:length(x), function(r) 
    sd( (f(x[1:r])+f(xprime[1:r])) /2 ) )
}

AV_ergodic = function(n_max, a=0, b=1/20){
  
  x        = runif(n_max,a,b)
  xprime   = a+b-x
  h = (f(x) + f(xprime))/2
  thetahat = (b-a) * cumsum(h)/c(1:n_max)
  
  sdthetahat = (b-a)/sqrt(2:n_max) * cumsd_AV(x,xprime)
  
  return(cbind(thetah = thetahat[-1], sd = sdthetahat))
}

plot_ergodic(MC_res, theta[1])


AV_res = AV_ergodic(n_max = n)
A = AV_ergodic(n_max = n)
lines(A[,1],type="l",lwd=2,col="orange")
lines(A[,1] + 1.96 * A[,2],
      type="l",col="red",lwd=2)
lines(A[,1] - 1.96 * A[,2],
      type="l",col="red",lwd=2)

```


We obtain a more efficient point and interval estimation with a smaller standard deviation and nearest estimation of the integral. In yellow we can see the estimation and in red the corresponding interval. The yellow line is really near the \textit{real} line and this value is in the center of the interval that is close than the blue one. We can see numerically too:

```{r}
# Numerically
MC_res[n-1,]
AV_res[n-1,]
theta[1]
```
The first \texttt{thetah} is the estimation with MC very smaller than the last value that is the output of \texttt{integrate} function while the second approximation is closer than the \textit{real} value anb the second standard deviation is smaller than the first one too.

Finally, we can compare our results to the output of \texttt{R} function that should give the probability that we are looking for, \texttt{pnorm} but it gives the asymptotic result 0. Our method gives a more accurate result.

```{r}
1-pnorm(20,0,1)
```
