---
title: "Assignment CS montecarlo"
author: "Patrick Poetto"
date: "`r Sys.Date()`"
output: 
pdf_document:
  latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/pat/Desktop")
library(knitr)
library(ggplot2)
```

* Montecarlo Simulations I 
** Point a) 

An old and naive algorithm for the generation of Normally distributed random numbers is the following:

$$
U_1, \ldots, U_{12} \sim U\left(-\frac{1}{2}, \frac{1}{2}\right); \qquad Z = \sum_{i=1}^{12} U_i
$$

The algorithm generates twelve independent unimoform variables between $$(-1/2, 1/2)$$ and then sets Z as the sum of them. The rationale here is that 12 realizations are usually enough to exploit the CLT.
Let's firstly consider the expected value of Z: since our variables are independent and uniformly distributed between $[-1/2, 1/2]$ the expected value of the sum is the sum of the expected value by linearity of the operator.   

$$
\begin{aligned}
E(Z) &= E(U_1 + U_2 + \cdots + U_{12}) = E(U_1) + E(U_2) + \cdots + E(U_{12}) = 0 + 0 + \cdots + 0 = 0
\end{aligned}
$$
Beacuse the expected value of a Uniform random variable is $$E(U) = (a+b)/2$$ so it is zero in the case of a Uniform centered in the origin. More formally we can write: 

$$
\begin{aligned}
E\left(\sum_{i=1}^{12}U_i\right) = \sum_{i=1}^{12}E(U_i) \
= \sum_{i=1}^{12}0 \ = 0
\end{aligned}
$$

Looking at the variance of Z:

$$
\begin{aligned}
\text{Var}(Z) &= \text{Var}(U_1 + U_2 + \cdots + U_{12}) \\
&= \text{Var}(U_1) + \text{Var}(U_2) + \cdots + \text{Var}(U_{12}) \\
&\qquad+ 2\text{Cov}(U_1, U_2) + 2\text{Cov}(U_1, U_3) + \cdots + 2\text{Cov}(U_{11}, U_{12})
\end{aligned}
$$

Where $\text{Cov}(U_i, U_j)$ denotes the covariance between $U_i$ and $U_j$.

Since $U_1, U_2, \ldots, U_{12}$ are again independent we have:

$$
\begin{aligned}
\text{Cov}(U_i, U_j) = 0 \quad \forall i \neq j
\end{aligned}
$$

Therefore,

$$
\begin{aligned}
\text{Var}(Z) &= \text{Var}(U_1) + \text{Var}(U_2) + \cdots + \text{Var}(U_{12}) \\
&= 12\cdot \text{Var}(U_1)
\end{aligned}
$$

Because the variance of the Uniformm random variable is $$V(U) = \frac{(b-a)^2}{12}$$, we have::

$$
\begin{aligned}
\text{Var}(U_1) &= \left(\frac{\frac{1}{2}-\left(-\frac{1}{2}\right)}{\sqrt{12}}\right)^2 \\
&= \frac{1}{12}
\end{aligned}
$$

Therefore,

$$
\begin{aligned}
\text{Var}(Z) &= 12 \cdot \frac{1}{12} \\
&= 1
\end{aligned}
$$

So we proved that $E(Z) = 0$ e $\text{Var}(Z) = 1$.

** Point b) 

Using histograms, compare the above Normal generator with the Box–Mueller algorithm. Comment on the results, and pay particular attention to tail probabilities (e.g., what happens to the estimate of P (Z ≥ 3)).

```{r}
# Box-Mueller
set.seed(123)
u1 = runif(100000)
u2 = runif(100000)

unif.0.2pi = 2*pi*u2
exp.1over2 = - 2 * log(1-u1)

X1 = sqrt( - 2 * log(1-u1)) * cos(2*pi*u2)
X2 = sqrt( - 2 * log(1-u1)) * sin(2*pi*u2)

# Uniform sum generator 
set.seed(123)
n = 100000
U_i = matrix(runif(n*12, min=-1/2, max=1/2), nrow=n)
Z = rowSums(U_i)

# R rnorm generator
set.seed(123)
N = rnorm(100000, 0, 1)

par(mfrow = c(3, 2))
hist(X1, breaks=50, col="blue", main="X1 distribution via Box Mueller algorithm")
hist(X1, breaks=100, col="green", main="Tail", xlim=c(3, max(X1)), ylim=c(0, 60))
hist(Z, breaks=50, col="blue", main="Z distribution via naive generator")
hist(Z, breaks=100, col="green", main="Tail", xlim=c(3, max(Z)), ylim=c(0, 60))
hist(N, breaks=50, col="blue", main="N distribution via R rnorm generator")
hist(N, breaks=100, col="green", main="Tail", xlim=c(3, max(N)), ylim=c(0, 60))
```


```{r}
X1_tail = mean(X1>3)
Z_tail = mean(Z>3)
N_tail = mean(N>3)
Norm = 1-pnorm(3, 0, 1)
X1_tail; Z_tail; N_tail; Norm

```

It can be concluded that the Box Muller algorithm and our generator built by a sum of Uniform distributed random variables are both able to properly approximate a sample drawn from a Normal distribution, but for large numbers (n = 100.000) the Box Mueller Algorithm can better approximate the tails. The R built-in random sampling algorithm "rnorm" is even more precise though. 

Let's now try to see the behaviour of our generators drawing S = 1000 samples of size n = 1000 for each method and comparing the results stability. 

```{r}
set.seed(123)

# Box Mueller 
n <- 10000
S1 <- matrix(0, nrow=n, ncol=1000)
for (i in 1:1000) {
  u1 <- runif(n)
  u2 <- runif(n)
  S1[, i] <- sqrt(-2*log(u1))*cos(2*pi*u2)
}

# Uniform Sum
S2 <- matrix(0, nrow=n, ncol=1000)
for (i in 1:1000) {
  U_i <- matrix(runif(n*12, min=-1/2, max=1/2), nrow=n)
  S2[, i] <- rowSums(U_i)
}

# rnorm
S3 <- matrix(rnorm(n*1000, 0, 1), nrow=n, ncol=1000)
```

Now we can compute the tail probability for each sample of each method.

```{r}
prob_S1 <- apply(S1, 2, function(x) mean(x > 3))
prob_S2 <- apply(S2, 2, function(x) mean(x > 3))
prob_S3 <- apply(S3, 2, function(x) mean(x > 3))
```

```{r}
# dataframe with the sampled distributions for each method
df <- data.frame(
  sample = rep(c("S1", "S2", "S3"), each = 1000),
  prob = c(prob_S1, prob_S2, prob_S3)
)
df$sample <- factor(df$sample, levels = c("S1", "S2", "S3"))

# plot 
ggplot(df, aes(x=sample, y=prob, fill=sample)) +
  geom_boxplot() +
  scale_fill_manual(values = c("red", "blue", "green")) +
  labs(x="Sample", y="P(X1, Z, N > 3") +
  ggtitle("Boxplots for each vector of 1000 probabilities") +
  scale_x_discrete(labels = c("Box Mueller", "Uniform Sum", "Rnorm"))
```

** Point d) 
```{r}
tail_prob_plot <- function(sample_size) {
  
  # generate samples
  set.seed(123)
  u1 = runif(sample_size)
  u2 = runif(sample_size)
  X1 = sqrt(-2*log(u1))*cos(2*pi*u2)
  Z = rowSums(matrix(runif(sample_size*12, min=-1/2, max=1/2), nrow=sample_size))
  N = rnorm(sample_size, 0, 1)
  
  # define range of theta values
  theta <- seq(0, 5, length.out = 100)
  
  # initialize vector for storing probabilities
  prob_X1 <- rep(0, length(theta))
  prob_Z <- rep(0, length(theta))
  prob_N <- rep(0, length(theta))
  
  # compute probabilities for each value of theta
  for (i in 1:length(theta)) {
    prob_X1[i] <- sum(X1 >= theta[i])/sample_size
    prob_Z[i] <- sum(Z >= theta[i])/sample_size
    prob_N[i] <- sum(N >= theta[i])/sample_size
  }
  
  # plot results
  plot(theta, prob_X1, type="l", col="red", xlab = "theta", ylab = "P(Z >= theta)", 
       main = paste("Upper Tail Probability for Sample Size =", sample_size))
  lines(theta, prob_Z, type="l", col="blue")
  lines(theta, prob_N, type="l", col="green")
  legend("topright", legend = c("Box-Mueller", "Uniform Sum", "Rnorm"), col = c("red", "blue", "green"), lty = 1)
}
```

```{r}
tail_prob_plot(1000)

```

** Point e) 
```{r}
# define the function to compute the upper tail probability 
tail_prob <- function(theta) {
  return(pnorm(-theta))
}

# range for theta 
theta <- seq(0, 5, length.out = 6)

# compute the upper tail probabilities for each theta 
true_prob <- tail_prob(theta)
mc_prob_X1 <- sapply(theta, function(t) sum(X1 >= t) / length(X1))
mc_prob_Z <- sapply(theta, function(t) sum(Z >= t) / length(Z))
mc_prob_N <- sapply(theta, function(t) sum(N >= t) / length(N))

# plot using matplot 
matplot(theta, cbind(true_prob, mc_prob_X1, mc_prob_Z, mc_prob_N),
        type = "l", lty = c(1,2,2,2), lwd = c(2,1,1,1),
        xlab = "Theta", ylab = "P(Z >= theta)",
        main = "Upper Tail Probability Comparison",
        col = c("black", "red", "blue", "green"))
legend("topright", c("True Distribution", "X1", "Z", "N"),
       lty = c(1,2,2,2), lwd = c(2,1,1,1),
       col = c("black", "red", "blue", "green"))

```

