---
title: "Assignment-2"
author: "Manuel Bottino"
date: "`r Sys.Date()`"

output:
  pdf_document: 
    toc: true
    number_sections: true
    latex_engine: pdflatex
fig_caption: yes
urlcolor: blue
header-includes:
  - \usepackage{amssymb}
editor_options: 
  markdown: 
    wrap: 72
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(magrittr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
library(bookdown)
library(plotly)
library(rstanarm)
library(Cairo)
library(rgl)
library(plot3D)
library(effects)
library(MASS)
library(car)
library(faraway)
library(leaps)
library(ROCR)
library(RColorBrewer)
library(ISLR2)
library(e1071)
library(class)
library(reshape2)
library(sads)
library(knitr)
library(latex2exp)
library(GGally)
library(combinat)
library(superheat)
library(stringr)
library(purrr)
library(tidyr)
library(lubridate)
library(kableExtra)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Excercise 1 - Risiko! is back, with friends




## Point a)
```{r}
df <- readRDS("~/Documents/GitHub/Computational-stat/Assignment 2/games_preprocessed.RDS")

plot(df$average_rating, df$yearpublished) # to see whether old board_games tend to be preferred more because of their assessed popularity

ggplot(data=df, aes(y=playingtime, x=as.factor(minplayers)))+ # As more player play are needed to start the game,                                                                 the average duration increases
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE)
```
## Point b)
```{r}
df= df[,-1:-2]
df=data.frame(scale(df))
```

```{r}
sample_cov=cov(df)
cor(df)
# We can get the same result using cov2cor
cov2cor(sample_cov)
```
Since we have scaled all the columns with variance equal to 1, it implies that the covariance-variance matrix is equal to the correlation matrix. Fonte: https://math.stackexchange.com/questions/3780344/under-what-conditions-will-the-covariance-matrix-be-identical-to-the-correlation

## Point c)
```{r}
B=10

cov_var_sample=array(NA, dim=c(ncol(df),ncol(df),B))

for(i in 1:B){
  cov_var_sample[,,i]=cov(slice_sample(df,n=nrow(df), replace=TRUE))
}

theta_hat_bar=apply(cov_var_sample, c(1, 2), mean)
se_cov_var=apply(cov_var_sample, c(1, 2), sd)

```

Confidence intervals
1. A first method consist on using the fact that, as ths sample size n grows large $\frac{\hat \theta - \theta}{SE_B(\hat \theta)} \approx N(0,1)$
```{r}
lower_bound=sample_cov+qnorm(c(0.025), mean=0, sd=1)*se_cov_var
upper_bound=sample_cov+qnorm(c(0.975), mean=0, sd=1)*se_cov_var
```

2. Use the quantile of the bootstrap sampling distribution, this method is considered more robust than the one before
```{r}
boot_sample_variance <- data.frame(matrix(nrow = 6, ncol = 6))
colnames(boot_sample_variance) <- c("maxplayers", "playingtime", "minage", "average_rating", "total_owners", "average_weight")

for(i in 1:6){
  for(j in 1:6){
    boot_sample_variance[i,j]=list(list(cov_var_sample[i,j,]))
  }
}
boot_sample_variance_ci <- data.frame(matrix(nrow = 6, ncol = 6))
colnames(boot_sample_variance_ci) <- c("maxplayers", "playingtime", "minage", "average_rating", "total_owners", "average_weight")

for(i in 1:6){
  for(j in 1:6){
    boot_sample_variance_ci[i,j]=list(list(quantile(boot_sample_variance[i,j][[1]], probs=c(0.025,0.975))))
  }
}
```

3. Using as assumption the fact that the behavior of the bias $\theta - \hat \theta$ is the same as $\hat \theta - \hat \theta^*$, we can compute the confidence interval of the bias-corrected estimate
```{r}
boot_sample_variance_ci_bias <- data.frame(matrix(nrow = 6, ncol = 6))
colnames(boot_sample_variance_ci) <- c("maxplayers", "playingtime", "minage", "average_rating", "total_owners", "average_weight")

for(i in 1:6){
  for(j in 1:6){
    boot_sample_variance_ci_bias[i,j]=list(list(2*sample_cov[i,j]-quantile(boot_sample_variance[i,j][[1]], probs=c(0.975,0.025))))
  }
}

```


```{r}

plot_boot_sample_ci = function(row, column){
  ggplot()+
  geom_histogram(aes(x=boot_sample_variance[row,column][[1]]),alpha=.4,col=4,fill="white",bins=25)+
  geom_vline(aes(xintercept = sample_cov[row,column]) ,col=1, lwd = 1)+
  geom_vline(aes(xintercept = boot_sample_variance_ci[row, column][[1]][1]) ,col="orange", lwd = 1)+
  geom_vline(aes(xintercept = boot_sample_variance_ci[row, column][[1]][2]) ,col="orange", lwd = 1)+
  geom_vline(aes(xintercept = boot_sample_variance_ci_bias[row, column][[1]][1]) ,col="red", lwd = 1)+
  geom_vline(aes(xintercept = boot_sample_variance_ci_bias[row, column][[1]][2]) ,col="red", lwd = 1)+
  geom_vline(aes(xintercept = lower_bound[row, column]) ,col="green", lwd = 1)+
  geom_vline(aes(xintercept = upper_bound[row, column]) ,col="green", lwd = 1)+
  scale_linetype_manual( guide = guide_legend())+
  xlab(TeX("$\\beta$"))+
  ylab("Frequency") +
  theme(text = element_text(size=14), legend.position = "none")

}

 grid.arrange(plot_boot_sample_ci(2,3),plot_boot_sample_ci(1,1))
```





## Point d)

```{r}
#We know that these two values are equal
sum(eigen(sample_cov)$values) & sum(diag(sample_cov))

j_star=6
theta_j_star=sum(eigen(sample_cov)$values[1:j_star])/sum(eigen(sample_cov)$values)

theta=numeric(ncol(df))
for( i in 1:length(theta)){
  theta[i]=sum(eigen(sample_cov)$values[1:i]/sum(eigen(sample_cov)$values))
}

plot(ecdf(theta))
```


## Point e)

```{r}
sample(theta, replace=TRUE)
boot_eigen = function(){
  
}
```




## Point f)

Make things more feasible
```{r}
set.seed(abs(636-555-3226))
ind <- sample(1:nrow(df),5000,FALSE)
sub_data <- df[ind,]
```

```{r}
lm_game=lm(data=sub_data, average_rating ~ .)

res_sub_data=matrix(NA, 1, ncol(sub_data)+2)
res_sub_data[1,]=c(lm_game$coefficients, summary(lm_game)$r.squared,
               max((lm_game$coef["playingtime"]-lm_game$coef["minage"])/(lm_game$coef["minplayers"]+lm_game$coef["yearpublished"]),0 ))

colnames(res_sub_data)=c(colnames(sub_data), "R2" , "theta_hat")
res_sub_data[,"yearpublished"]-1
```

COMMENT


## Point g)
```{r}
plot(lm_game)
```
We can notice some features from this plots, useful for the choice of which bootstrap would be a sensitive choice. First of all the assumption of constant variance is not realistic in this case. Also the assumption of the errors distributed as a normal has problems, since we have a long tailed distribution. From this quick analysis of the model we can choose the bootstrap method more suited and explain why the others are not.
1. shuffle the errors -> this method relies on assumption that the chosen model fits the data well and the residuals have constant variance
2. Parametric bootstrap for logistic regression ->we are assuming that we know the distribution of the population which we don't
3. Resample the line of the two datasets -> this approach violates the assumption of constant (fixed) design matrix. 
however this applies perfectly to our case

```{r}
boot_reg_game <- function(data, B=200){
  
  n0 <- nrow(data)
  
  res <- replicate(B,{
    ind   <- sample(1:n0, n0, TRUE)
    Dstar <- data[ind,]
    Mstar <- lm(data=Dstar, average_rating ~ .)
    
    c(Mstar$coef, summary(Mstar)$r.squared, max((Mstar$coef["playingtime"]-Mstar$coef["minage"])/(Mstar$coef["minplayers"]+Mstar$coef["yearpublished"]),0 ))
  })
  
  t(res)
}

res_game_boot=boot_reg_game(data=sub_data)
colnames(res_game_boot)=c(colnames(sub_data), "R2" , "theta_hat")
```
Compute the estimates
```{r}
melted <- reshape2::melt(res_game_boot)
melted <- melted %>% group_by(Var2) %>% mutate( boot_sd = sd(value),
                                         boot_lowerbound = quantile(value, .025), boot_upperbound = quantile(value, .975),
                                         boot_mean = mean(value), bias=boot_mean-res_sub_data[,as.character(Var2)]) %>% ungroup()
```



Plot the results
```{r}


ggplot( melted )+
  geom_point(aes(x=Var1,y=value),alpha=.4,col=4)+
  geom_hline(aes(yintercept = boot_upperbound) ,col=2, lwd = 1)+
  geom_hline(aes(yintercept = boot_lowerbound) ,col=2, lwd = 1)+
  geom_hline(aes(yintercept = boot_mean) ,col=2, lwd = 1, lty=3)+
  facet_wrap(~Var2,scales = "free")+
  xlab("Bootstrap replication")+
  ylab(TeX("$\\beta$, $R^2$ and \\theta")) +
  theme(text = element_text(size=14), legend.position = "none")

ggplot( melted )+
  geom_histogram(aes(x=value),alpha=.4,col=4,fill="white",bins=25)+
  geom_vline(aes(xintercept = boot_upperbound) ,col=2, lwd = 1)+
  geom_vline(aes(xintercept = boot_lowerbound) ,col=2, lwd = 1)+
  geom_vline(aes(xintercept = boot_mean) ,col=2, lwd = 1, lty=3)+
  facet_wrap(~Var2,scales = "free")+
  xlab(TeX("$\\beta$ and AIC"))+
  ylab(TeX("$\\beta$, $R^2$ and \\theta")) +
  theme(text = element_text(size=14), legend.position = "none")



```



## Point h)
Jackknife approach
```{r}

jack_reg_game = function(data, B){
  
  n0 <- nrow(data)
  
  res=matrix(NA, n0, ncol(data)+2)
for(i in 1:n0){
  Mjack=lm(data=data[-i,], average_rating ~ .)
  res[i,]=c(Mjack$coef, summary(Mjack)$r.squared, max((Mjack$coef["playingtime"]-Mjack$coef["minage"])/(Mjack$coef["minplayers"]+Mjack$coef["yearpublished"]),0 ))
}
  
  
  
  return(res)
}

res_game_jack=jack_reg_game(sub_data, 10)
colnames(res_game_jack)=c(colnames(sub_data), "R2" , "theta_hat")
```
Compute the jackknife estimate, bias-corrected estimate and standard error

```{r}
melted <- reshape2::melt(res_game_jack)
melted <- melted %>% group_by(Var2) %>% mutate( boot_sd = sd(value),
                                         boot_lowerbound = quantile(value, .025), boot_upperbound = quantile(value, .975),
                                         boot_mean = mean(value), bias=boot_mean-res_sub_data[,as.character(Var2)]) %>% ungroup()
```


Plot the results
```{r}

ggplot( melted )+
  geom_point(aes(x=Var1,y=value),alpha=.4,col=4)+
  geom_hline(aes(yintercept = boot_upperbound) ,col=2, lwd = 1)+
  geom_hline(aes(yintercept = boot_lowerbound) ,col=2, lwd = 1)+
  geom_hline(aes(yintercept = boot_mean) ,col=2, lwd = 1, lty=3)+
  facet_wrap(~Var2,scales = "free")+
  xlab("Bootstrap replication")+
  ylab(TeX("$\\beta$, $R^2$ and \\theta")) +
  theme(text = element_text(size=14), legend.position = "none")

ggplot( melted )+
  geom_histogram(aes(x=value),alpha=.4,col=4,fill="white",bins=25)+
  geom_vline(aes(xintercept = boot_upperbound) ,col=2, lwd = 1)+
  geom_vline(aes(xintercept = boot_lowerbound) ,col=2, lwd = 1)+
  geom_vline(aes(xintercept = boot_mean) ,col=2, lwd = 1, lty=3)+
  facet_wrap(~Var2,scales = "free")+
  xlab(TeX("$\\beta$ and AIC"))+
  ylab(TeX("$\\beta$, $R^2$ and \\theta")) +
  theme(text = element_text(size=14), legend.position = "none")


```



# Exercise 2 - We need some music!

```{r echo=FALSE}
df2 = readRDS("~/Documents/GitHub/Computational-stat/Assignment 2/spot.RDS")
```


## Point a)
The dataset we're working with is a nested from a bigger version comprehending 2072 observations for 18 variables providing data of a Spotify tracks sample. Data are provided by the Spotify for Developers API (source: https://developer.spotify.com/documentation/web-api/reference/get-audio-features). 

```{r echo=FALSE}
df2 = subset(df2, select = c(acousticness, danceability, energy, instrumentalness, liveness, speechiness, valence, pop, loudness, duration_ms, tempo, n, explicit, first_auth, name, release_date, id)) # re ordering variables for better explanatory purposes

subset1 <- subset(df2, select = c(acousticness, danceability, energy, instrumentalness, 
                                  liveness, speechiness))
subset2 <- subset(df2, select = c(valence, pop, loudness, duration_ms, tempo, n))

kable(summary(subset1))
kable(summary(subset2))
```

Note that factors and characters are excluded from the summary. 
The first 8 variables are all synthetic float indexes from 0 to 1 describing one feature of the track, sometime based on musical features like path, tempo, beat, stability, strenght, regularity, and sometime based on elements which may come out during the analysis (background noise for liveness, sound for acousticness). These ones measure for example danceability or energy as well as liveness or acousticness. Then there are three numerical variables which are measures of duration, loudness and the tempo of the song and a factor which tells if the track is explicit (0) or not (1). Four qualitative variables then tell us the original author, the artist who actually published the track, and an ID of the track itself. Variable n is eventually the numer of songs by the same artist in this dataset. 

```{r echo=FALSE, fig.height=4, fig.width=7}
# Select synthetic index variables and create dataframe
columns = c("acousticness", "danceability", "energy", "instrumentalness", "liveness", "speechiness", "valence", "pop")
indexes = df2[, columns]
# Colors
colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

# Boxplots
boxplots <- stack(indexes)
ggplot(boxplots, aes(x = ind, y = values)) +
  geom_boxplot(fill = colors) +
  labs(x = "Variable", y = "Value") +
  ggtitle("Boxplots of synthetic indexes") +
  theme_minimal()
```

From these boxplots we can have an idea of the distributions of each feature among the tracks. For example we can say that most songs are likely to be non live, acoustic or spoken. Danceability, energy and popularity are distributed a little above 0.5 and the first two present a fatter left tail and viceversa for the third. The variable valence seems to be the most symmetric around the middle value, with equally weighted tails. 

```{r echo=FALSE, fig.height=4.2, fig.width=8}
# Correlation Matrix dataframe
columns2 <- c("acousticness", "danceability", "energy", "instrumentalness", "liveness", "speechiness", "valence", "pop", "loudness", "duration_ms", "tempo")
data = df2[, columns2]
cor_matrix = cor(data)
cor_df <- melt(cor_matrix)

# Shortening variable names and re ordering variables 
cor_df$Var2 <- str_wrap(cor_df$Var2, width = 10)
cor_df$Var1 <- factor(cor_df$Var1, levels = columns2)
cor_df$Var2 <- factor(cor_df$Var2, levels = rev(columns2))

# Heatmap
ggplot(data = cor_df, aes(x = Var2, y = Var1, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "white", size = 3) +
  scale_fill_gradientn(colors = c("#440154FF", "#3B528BFF", "#21908CFF", "#5DC863FF", "#FDE725FF")) +
  theme_minimal() +
  labs(title = "Correlation Matrix",
       x = "",
       y = "",
       fill = "Correlation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_cartesian(clip = "off", expand = FALSE) +
  theme(axis.title.y = element_text(margin = margin(r = 10))) +
  theme(axis.text.y = element_text(margin = margin(r = 10))) +
  theme(axis.title.x = element_text(margin = margin(t = 10))) +
  theme(axis.text.x = element_text(margin = margin(t = 10))) +
  theme(panel.border = element_blank())
```

The correlation matrix plotted on a heatmap shows that most variables are almost uncorrelated with each other. The variable loudness is strongly positive related with energy and negative related with acousticness, which perfectly makes sense as well as the fact that energy and acousticness are negative related. Another reasonable result is that valence (negativity or positivity of the song) has some correlation with danceability and energy.

```{r echo=FALSE}
# Histograms
par(mfrow = c(2, 3))
hist(df2$loudness, col = "#56B4E9", main = "Loudness", xlab="Loudness")
hist(df2$duration_ms, col = "#F0E442", main = "Duration (ms)", xlab="Duration")
hist(df2$tempo, col = "#D55E00", main = "Tempo", xlab="Tempo")
barplot(table(df2$n), col = "#CC79A7", main = "n")
barplot(table(df2$explicit), col = "#FF6F91", main = "Explicit")
barplot(table(df2$release_date), col = "#999999", main = "Release Date")
```

The histograms show the distribution for numerical variables.

```{r echo=FALSE, fig.height=2.4, fig.width=8}
# Top 3 authors
top_authors <- df2 %>%
  group_by(first_auth) %>%
  summarise(n = n()) %>%
  top_n(3, n) %>%
  arrange(desc(n))

# Dataset filtering
filtered_data <- df2 %>%
  filter(first_auth %in% top_authors$first_auth)

# Variables list splitted
group1 <- c("acousticness", "danceability", "energy", "instrumentalness")
group2 <- c("liveness", "speechiness", "valence", "pop")

# Mean of each variable for each author
mean_values_group1 <- filtered_data %>%
  group_by(first_auth) %>%
  summarise(acousticness = mean(acousticness),
            danceability = mean(danceability),
            energy = mean(energy),
            instrumentalness = mean(instrumentalness)) %>%
  pivot_longer(cols = -first_auth, names_to = "Variable", values_to = "MeanValue")

mean_values_group2 <- filtered_data %>%
  group_by(first_auth) %>%
  summarise(liveness = mean(liveness),
            speechiness = mean(speechiness),
            valence = mean(valence),
            pop = mean(pop)) %>%
  pivot_longer(cols = -first_auth, names_to = "Variable", values_to = "MeanValue")

# Histograms
plot_group1 <- ggplot(data = mean_values_group1, aes(x = first_auth, y = MeanValue, fill = Variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Mean value by author",
       x = "Author",
       y = "Mean value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_minimal() +
  scale_fill_manual(values = alpha(RColorBrewer::brewer.pal(n = 8, name = "Set2"), 0.7))


plot_group2 <- ggplot(data = mean_values_group2, aes(x = first_auth, y = MeanValue, fill = Variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Mean values by author",
       x = "Author",
       y = "Mean value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_minimal() +
  scale_fill_manual(values = alpha(RColorBrewer::brewer.pal(n = 8, name = "Set2"), 0.7))


# Visualizza i grafici
par(mfrow = c(2, 1))
plot_group1
plot_group2

```

Histograms above show the mean value of our indexes for the top three artists (based on number of songs in the data) to have a qualitative idea of at least the three authors that contributed the most to the data. 

```{r echo=FALSE, fig.height=3, fig.width=8, warning=FALSE}
# Compute frequencies for first_auth and reorder authors 
first_auth_freq <- df2 %>%
  count(first_auth) %>%
  arrange(desc(n)) %>%
  mutate(first_auth = reorder(first_auth, desc(n)))

# first_author histogram
bar_plot_first_auth <- ggplot(first_auth_freq, aes(x = first_auth, y = n, fill = "Author")) +
  geom_bar(stat = "identity") +
  labs(title = "First Author frequency",
       x = "Author",
       y = "Frequency",
       fill = "Author frequency") +
  scale_fill_manual(values = alpha("green", 0.5), guide="legend") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Exract the year of release_date
df2$anno_release <- year(ymd(df2$release_date))

# Compute frequencies for year of release
release_date_freq <- df2 %>%
  count(anno_release) %>%
  arrange(desc(n))

# Histogram for year of release
bar_plot_release_date <- ggplot(release_date_freq, aes(x = as.character(anno_release), y = n, fill = "Release year")) +
  geom_bar(stat = "identity") +
  labs(title = "Release year distribution",
       x = "Release year",
       y = "Frequency",
       fill = "Release year") +
  scale_fill_manual(values = alpha("red", 0.5), guide = "legend") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_cartesian(ylim = c(40, max(release_date_freq$n)))

bar_plot_first_auth
bar_plot_release_date
```

We plotted the frequencies of authors in the dataset and frequencies of release per year, to have an idea of how the two most relevant qualitative variables are distributed, aggregating tracks among their author and over the years of release. 

## Point b)

## Point c)

## Point d)

## Point e)

## Point f)

## Point g)




